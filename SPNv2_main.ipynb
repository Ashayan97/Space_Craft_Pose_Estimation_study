{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Spliting train and test data"
   ],
   "metadata": {
    "id": "bkWzfNpOQOIJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Opening JSON file\n",
    "f = open('train.json')\n",
    "\n",
    "# Reading only the first 6000 data without background image\n",
    "data = json.load(f)[:6000]\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "# dummy_train = data[:10]\n",
    "train = data[:int(len(data)*0.7)]\n",
    "validation = data[int(len(data)*0.7):int(len(data)*0.9)]\n",
    "test = data[-int(len(data)*0.1):]\n",
    "def key_return(elem):\n",
    "    return elem['filename']\n",
    "\n",
    "train.sort(key=key_return)\n",
    "test.sort(key=key_return)\n",
    "validation.sort(key=key_return)\n",
    "# dummy_train.sort(key=key_return)\n",
    "#\n",
    "# out_dummy = open(\"new_labels/without_back/dummy_train.json\", \"w\")\n",
    "#\n",
    "# json.dump(dummy_train, out_dummy)\n",
    "#\n",
    "# out_dummy.close()\n",
    "\n",
    "out_train = open(\"Datasets/speed/synthetic/train.json\", \"w\")\n",
    "\n",
    "json.dump(train, out_train)\n",
    "\n",
    "out_train.close()\n",
    "\n",
    "\n",
    "out_test = open(\"Datasets/speed/synthetic/test.json\", \"w\")\n",
    "\n",
    "json.dump(test, out_test)\n",
    "\n",
    "out_test.close()\n",
    "\n",
    "\n",
    "out_validation = open(\"Datasets/speed/synthetic/validation.json\", \"w\")\n",
    "\n",
    "json.dump(validation, out_validation)\n",
    "\n",
    "out_validation.close()\n",
    "\n",
    "f.close()"
   ],
   "metadata": {
    "id": "9qHGSgPyPYa_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing"
   ],
   "metadata": {
    "id": "tUNmyv2tQTht"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TB_77r0nYZrH",
    "outputId": "da1c867e-c495-4c74-9b38-6a7482083d0f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading JSON file from Datasets/speed/synthetic/validation.json...\n",
      "Label CSV file will be saved to Datasets/synthetic/labels/validation.csv\n",
      "Resized images will be saved to Datasets/synthetic/images_768x512_RGB\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 600/600 [00:17<00:00, 35.01it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done\n",
      "\n",
      "\n",
      "Reading JSON file from Datasets/speed/synthetic/train.json...\n",
      "Label CSV file will be saved to Datasets/synthetic/labels/train.csv\n",
      "Resized images will be saved to Datasets/synthetic/images_768x512_RGB\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5400/5400 [02:34<00:00, 34.92it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Copyright (c) 2022 SLAB Group\n",
    "Licensed under MIT License (see LICENSE.md)\n",
    "Author: Tae Ha Park (tpark94@stanford.edu)\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "\n",
    "import easydict\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import _init_paths\n",
    "\n",
    "from core.config import cfg, update_config\n",
    "from core.utils.utils import load_camera_intrinsics, load_tango_3d_keypoints\n",
    "from core.utils.postprocess import project_keypoints\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    update_config(cfg, args)\n",
    "\n",
    "    datadir = os.path.join(cfg.DATASET.ROOT)\n",
    "\n",
    "    # Read labels from JSON file\n",
    "    jsonfile = args.jsonfile\n",
    "    print(f'Reading JSON file from {jsonfile}...')\n",
    "    with open(jsonfile, 'r') as f:\n",
    "        labels = json.load(f) # list\n",
    "\n",
    "    # Read camera\n",
    "    camera = load_camera_intrinsics(cfg.DATASET.CAMERA)\n",
    "\n",
    "    # Read Tango 3D keypoints\n",
    "    keypts3d = load_tango_3d_keypoints(cfg.DATASET.KEYPOINTS) # (11, 3) [m]\n",
    "\n",
    "    # Where to save CSV?\n",
    "    if cfg.DATASET.DATANAME == 'speed':\n",
    "        domain, split = args.jsonfile.split('/')[-2:]\n",
    "    elif cfg.DATASET.DATANAME == 'prisma25':\n",
    "        domain, split = '', args.jsonfile\n",
    "    elif 'shirt' in cfg.DATASET.DATANAME:\n",
    "        traj, domain, split = args.jsonfile.split('/')\n",
    "        domain = traj + '/' + domain\n",
    "    else:\n",
    "        raise NotImplementedError('Only accepting speedplus and prisma25')\n",
    "    outdir = os.path.join(datadir, domain, 'labels')\n",
    "    if not os.path.exists(outdir): os.makedirs(outdir)\n",
    "    csvfile = os.path.join(outdir, split.replace('json', 'csv'))\n",
    "    print(f'Label CSV file will be saved to {csvfile}')\n",
    "\n",
    "    # Where to save resized image?\n",
    "    imagedir = os.path.join(datadir, domain,\n",
    "            f'images_{cfg.DATASET.INPUT_SIZE[0]}x{cfg.DATASET.INPUT_SIZE[1]}_RGB')\n",
    "    if not os.path.exists(imagedir): os.makedirs(imagedir)\n",
    "    print(f'Resized images will be saved to {imagedir}')\n",
    "\n",
    "    if args.load_masks:\n",
    "        maskdir = os.path.join(datadir, domain,\n",
    "            f'masks_{int(cfg.DATASET.INPUT_SIZE[0]/cfg.DATASET.OUTPUT_SIZE[0])}x{int(cfg.DATASET.INPUT_SIZE[1]/cfg.DATASET.OUTPUT_SIZE[0])}')\n",
    "        if not os.path.exists(maskdir): os.makedirs(maskdir)\n",
    "        print(f'Resized masks will be saved to {maskdir}')\n",
    "\n",
    "    # Open\n",
    "    csv = open(csvfile, 'w')\n",
    "\n",
    "    for idx in tqdm(range(len(labels))):\n",
    "\n",
    "        # ---------- Read image & resize & save\n",
    "        filename = labels[idx]['filename']\n",
    "        image    = cv2.imread(os.path.join(datadir,cfg.DATASET.DATANAME, domain, 'images', filename), cv2.IMREAD_COLOR)\n",
    "        image    = cv2.resize(image, cfg.DATASET.INPUT_SIZE)\n",
    "        cv2.imwrite(os.path.join(imagedir, filename), image)\n",
    "\n",
    "        # ---------- Read mask & resize & save\n",
    "        if args.load_masks:\n",
    "            mask = cv2.imread(os.path.join(datadir,cfg.DATASET.DATANAME , domain, 'masks', filename), cv2.IMREAD_GRAYSCALE)\n",
    "            # print(os.path.join(datadir,cfg.DATASET.DATANAME , domain, 'masks', filename))\n",
    "            mask = cv2.resize(mask, [int(s / cfg.DATASET.OUTPUT_SIZE[0]) for s in cfg.DATASET.INPUT_SIZE])\n",
    "            cv2.imwrite(os.path.join(maskdir, filename), mask)\n",
    "\n",
    "\n",
    "        # ---------- Read labels\n",
    "        if args.load_labels:\n",
    "            q_vbs2tango = np.array(labels[idx]['q_vbs2tango'], dtype=np.float32)\n",
    "            r_Vo2To_vbs = np.array(labels[idx]['r_Vo2To_vbs_true'], dtype=np.float32)\n",
    "\n",
    "        # ---------- Project keypoints & origin\n",
    "        if args.load_labels:\n",
    "            # Attach origin\n",
    "            keypts3d_origin = np.concatenate((np.zeros((3,1), dtype=np.float32),\n",
    "                                            keypts3d), axis=1) # [3, 12]\n",
    "\n",
    "            keypts2d = project_keypoints(q_vbs2tango,\n",
    "                                        r_Vo2To_vbs,\n",
    "                                        camera['cameraMatrix'],\n",
    "                                        camera['distCoeffs'],\n",
    "                                        keypts3d_origin) # (2, 12)\n",
    "\n",
    "            keypts2d[0] = keypts2d[0] / camera['Nu']\n",
    "            keypts2d[1] = keypts2d[1] / camera['Nv']\n",
    "            # Into vector (x0, y0, kx1, ky1, ..., kx11, ky11)\n",
    "            keypts2d_vec = np.reshape(np.transpose(keypts2d), (24,))\n",
    "\n",
    "        # ---------- Bounding box labels\n",
    "        # If masks are available, get them from masks\n",
    "        # If not, use keypoints instead\n",
    "        if args.load_labels:\n",
    "            # if args.load_masks:\n",
    "            #     seg  = np.where(mask > 0)\n",
    "            #     # print(mask.shape)\n",
    "            #     xmin = np.min(seg[1]) / mask.shape[1]#camera['Nu']\n",
    "            #     ymin = np.min(seg[0]) / mask.shape[0]#camera['Nv']\n",
    "            #     xmax = np.max(seg[1]) / mask.shape[1]#camera['Nu']\n",
    "            #     ymax = np.max(seg[0]) / mask.shape[0]#camera['Nv']\n",
    "            # else:\n",
    "            xmin = np.min(keypts2d[0])\n",
    "            ymin = np.min(keypts2d[1])\n",
    "            xmax = np.max(keypts2d[0])\n",
    "            ymax = np.max(keypts2d[1])\n",
    "        # CSV row\n",
    "        row = [filename]\n",
    "\n",
    "        if args.load_labels:\n",
    "            row = row + [xmin, ymin, xmax, ymax] \\\n",
    "                      + q_vbs2tango.tolist() \\\n",
    "                      + r_Vo2To_vbs.tolist() \\\n",
    "                      + keypts2d_vec.tolist()\n",
    "\n",
    "        row = ', '.join([str(e) for e in row])\n",
    "\n",
    "        # Write\n",
    "        csv.write(row + '\\n')\n",
    "\n",
    "    csv.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "  files_name = ['validation.json','train.json','test.json']\n",
    "  for file_name in files_name:\n",
    "      args = easydict.EasyDict({\n",
    "      'cfg' : 'experiments/offline_train_full_config_phi3_BN_speed.yaml',\n",
    "      'jsonfile' : 'Datasets/speed/synthetic/'+file_name,\n",
    "      'no_masks' : False,\n",
    "      'no_labels' : False,\n",
    "      'load_masks' : True,\n",
    "      'load_labels' : True,\n",
    "      'opts' : []\n",
    "      })\n",
    "      main(args)\n",
    "  source = 'drive/MyDrive/Datasets/styles_768x512_RGB.zip'\n",
    "  destination  = 'Datasets/synthetic/styles_768x512_RGB.zip'\n",
    "\n",
    "  shutil.copy2(source, destination)\n",
    "\n",
    "  print('done\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train Code"
   ],
   "metadata": {
    "id": "Bxn0aL8jQolk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEO4ZoHL-69E"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Copyright (c) 2022 SLAB Group\n",
    "Licensed under MIT License (see LICENSE.md)\n",
    "Author: Tae Ha Park (tpark94@stanford.edu)\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import easydict \n",
    "\n",
    "\n",
    "import _init_paths\n",
    "# dist.dist_backend = 'gloo'\n",
    "from core.config import cfg, update_config\n",
    "from core.nets import build_spnv2\n",
    "from core.dataset import get_dataloader\n",
    "from core.solver import get_optimizer, adjust_learning_rate, get_scaler\n",
    "from core.engine.trainer import do_train\n",
    "from core.engine.inference import do_valid\n",
    "from core.utils.checkpoints import load_checkpoint, save_checkpoint\n",
    "from core.utils.utils import set_seeds_cudnn, setup_logger, create_logger_directories, \\\n",
    "    write_model_info, load_camera_intrinsics, load_tango_3d_keypoints\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # parser = argparse.ArgumentParser(description='Train SPNv2')\n",
    "    args = easydict.EasyDict({\n",
    "         'cfg' : 'experiments/offline_train_full_config_phi3_BN_speed.yaml',\n",
    "         'opts' : [],\n",
    "         'gpu' : None,\n",
    "         'world_size' : 1,\n",
    "         'rank' : 0,\n",
    "         'dist_url' : 'tcp://127.0.0.1:23456'\n",
    "         })\n",
    "    return args\n",
    "\n",
    "\n",
    "train_results = []\n",
    "valid_results = []\n",
    "\n",
    "def main(cfg,overfit_limit):\n",
    "    args = parse_args()\n",
    "    update_config(cfg, args)\n",
    "\n",
    "    cfg.defrost()\n",
    "    cfg.DIST.RANK = args.rank\n",
    "    cfg.freeze()\n",
    "\n",
    "    _, output_dir, log_dir = \\\n",
    "        create_logger_directories(cfg, phase='train', write_cfg_to_file=True)\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        warnings.warn('You have chosen a specific GPU. This will completely '\n",
    "                      'disable data parallelism.')\n",
    "\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "\n",
    "    main_worker(\n",
    "          args.gpu,\n",
    "          ngpus_per_node,\n",
    "          args,\n",
    "          output_dir,\n",
    "          log_dir,\n",
    "          overfit_limit\n",
    "        )\n",
    "\n",
    "\n",
    "def main_worker(gpu, ngpus_per_node, args, output_dir, log_dir, overfit_limit):\n",
    "    # Set all seeds & cudNN\n",
    "    set_seeds_cudnn(cfg, seed=cfg.SEED)\n",
    "\n",
    "    # GPU?\n",
    "    args.gpu = gpu\n",
    "    if args.gpu is not None:\n",
    "        print(f'Use GPU: {args.gpu} for training')\n",
    "\n",
    "    update_config(cfg, args)\n",
    "    args.distributed = False\n",
    "    # setup logger\n",
    "    logger = setup_logger(log_dir, args.rank, 'train', to_console=False)\n",
    "\n",
    "    # build network\n",
    "    model = build_spnv2(cfg)\n",
    "\n",
    "    # GPU device\n",
    "    if args.gpu is not None:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        device = torch.device('cuda', args.gpu)\n",
    "    else:\n",
    "        device = torch.device('cuda')\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # write model summary to file\n",
    "    write_model_info(model.module if args.distributed else model, log_dir)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = get_dataloader(cfg,\n",
    "                                  split='train',\n",
    "                                  distributed=args.distributed,\n",
    "                                  load_labels=True)\n",
    "    val_loader = get_dataloader(cfg,\n",
    "                                split='val',\n",
    "                                distributed=args.distributed,\n",
    "                                load_labels=True)\n",
    "\n",
    "    # Optimizer & scaler for mixed-precision training\n",
    "    optimizer = get_optimizer(cfg, model)\n",
    "    scaler = get_scaler(cfg)  # None if cfg.FP16 = False, cfg.CUDA = False\n",
    "\n",
    "    # Load checkpoints\n",
    "    checkpoint_file = osp.join(output_dir, f'checkpoint.pth.tar')\n",
    "    if cfg.AUTO_RESUME and osp.exists(checkpoint_file):\n",
    "        last_epoch, best_score = load_checkpoint(\n",
    "            checkpoint_file,\n",
    "            model,\n",
    "            optimizer,\n",
    "            scaler,\n",
    "            device)\n",
    "        begin_epoch = last_epoch\n",
    "    else:\n",
    "        begin_epoch = cfg.TRAIN.BEGIN_EPOCH\n",
    "        last_epoch = -1\n",
    "        best_score = 1e20\n",
    "\n",
    "    # For validation\n",
    "    camera = load_camera_intrinsics(cfg.DATASET.CAMERA)\n",
    "    keypts_true_3D = load_tango_3d_keypoints(cfg.DATASET.KEYPOINTS)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Main loop\n",
    "    # ---------------------------------------\n",
    "    score = best_score\n",
    "    is_best = False\n",
    "    is_final = False\n",
    "    counter = 0\n",
    "    for epoch in range(begin_epoch, cfg.TRAIN.END_EPOCH):\n",
    "        print('')\n",
    "        # Learning rate adjustment\n",
    "        adjust_learning_rate(optimizer, epoch, cfg)\n",
    "\n",
    "        # Single epoch training\n",
    "        train_results.append(do_train(epoch,\n",
    "                 cfg,\n",
    "                 model,\n",
    "                 train_loader,\n",
    "                 optimizer,\n",
    "                 log_dir=log_dir,\n",
    "                 device=device,\n",
    "                 scaler=scaler,\n",
    "                 rank=args.rank))\n",
    "        # Validate on validation set\n",
    "        score = do_valid(epoch,\n",
    "                         cfg,\n",
    "                         model,\n",
    "                         val_loader,\n",
    "                         camera,\n",
    "                         keypts_true_3D,\n",
    "                         log_dir=None,\n",
    "                         device=device)\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            is_best = True\n",
    "            counter = 0\n",
    "        else:\n",
    "            is_best = False\n",
    "            counter += 1\n",
    "        valid_results.append(score)\n",
    "\n",
    "        # Save\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'backbone': cfg.MODEL.BACKBONE.NAME,\n",
    "            'heads': cfg.MODEL.HEAD.NAMES,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_state_dict': model.module.state_dict() if args.distributed else model.state_dict(),\n",
    "            'best_score': best_score,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict() if scaler is not None else None\n",
    "        }, is_best, epoch + 1 == cfg.TRAIN.END_EPOCH or counter == overfit_limit, output_dir)\n",
    "        if counter == overfit_limit:\n",
    "          break\n",
    "if __name__ == '__main__':\n",
    "    main(cfg,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test Code"
   ],
   "metadata": {
    "id": "mdYgPB_FQsEd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5Vo-KWi7CAa",
    "outputId": "14621280-d709-4dde-cb16-b9d5403850ba"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing 001 [599/600] [  69.7 (  73.0) ms]\teffi_iou   0.93 (  0.85) \teffi_eR  10.47 ( 17.02) deg\teffi_eT   0.11 (  0.62) m\teffi_pose   0.22 (  0.35) "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Copyright (c) 2022 SLAB Group\n",
    "Licensed under MIT License (see LICENSE.md)\n",
    "Author: Tae Ha Park (tpark94@stanford.edu)\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path as osp\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "import _init_paths\n",
    "\n",
    "from core.config import cfg, update_config\n",
    "from core.nets   import build_spnv2\n",
    "from core.dataset import get_dataloader\n",
    "from core.engine.inference  import do_valid\n",
    "from core.utils.utils import set_seeds_cudnn, create_logger_directories, \\\n",
    "                        load_camera_intrinsics, load_tango_3d_keypoints\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Test on SPNv2')\n",
    "\n",
    "    # general\n",
    "    parser.add_argument('--cfg',\n",
    "                        help='experiment configure file name',\n",
    "                        required=True,\n",
    "                        type=str)\n",
    "\n",
    "    parser.add_argument('opts',\n",
    "                        help=\"Modify config options using the command-line\",\n",
    "                        default=None,\n",
    "                        nargs=argparse.REMAINDER)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "def main(cfg):\n",
    "    args = parse_args()\n",
    "    update_config(cfg, args)\n",
    "\n",
    "    # Load model to test\n",
    "    test_model = osp.join(cfg.OUTPUT_DIR, cfg.TEST.MODEL_FILE)\n",
    "    if not osp.exists(test_model) or osp.isdir(test_model):\n",
    "        test_model = 'outputs/efficientdet_d3/full_config/model_best.pth.tar'\n",
    "    cfg.defrost()\n",
    "    cfg.TEST.MODEL_FILE = test_model\n",
    "    cfg.freeze()\n",
    "\n",
    "    # Logger & directories\n",
    "    logger, output_dir, _ = create_logger_directories(cfg, 'test')\n",
    "\n",
    "    # Set all seeds & cudNN\n",
    "    set_seeds_cudnn(cfg, seed=cfg.SEED)\n",
    "\n",
    "    # GPU?\n",
    "    device = torch.device('cuda:0') if cfg.CUDA and torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # Complete network\n",
    "    model = build_spnv2(cfg)\n",
    "\n",
    "    # Load checkpoint\n",
    "    if cfg.TEST.MODEL_FILE:\n",
    "        model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE, map_location='cpu'), strict=True)\n",
    "        logger.info('   - Model loaded from {}'.format(cfg.TEST.MODEL_FILE))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Dataloaders\n",
    "    test_loader = get_dataloader(cfg, split='test', load_labels=True)\n",
    "\n",
    "    # For validation\n",
    "    camera = load_camera_intrinsics(cfg.DATASET.CAMERA)\n",
    "    keypts_true_3D = load_tango_3d_keypoints(cfg.DATASET.KEYPOINTS)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Main Test\n",
    "    # ---------------------------------------\n",
    "    score = do_valid(0,\n",
    "                     cfg,\n",
    "                     model,\n",
    "                     test_loader,\n",
    "                     camera,\n",
    "                     keypts_true_3D,\n",
    "                     valid_fraction=None,\n",
    "                     log_dir=output_dir,\n",
    "                     device=device)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Onlie Domain Refinment"
   ],
   "metadata": {
    "id": "ezFJgl80QuGM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EX5YtbAOxBNy",
    "outputId": "0b5b467e-6403-480f-db18-d45b5ed58c36"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ODR 001 [1024/1024] [ 134.5 ( 119.3) ms]\tent 7.79e-04 (2.31e-03) \n",
      "Testing 001 [599/600] [  61.1 (  62.1) ms]\teffi_iou   0.90 (  0.77) \teffi_eR   1.44 ( 14.11) deg\teffi_eT   0.02 (  2.41) m\teffi_pose   0.03 (  0.38) "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Copyright (c) 2022 SLAB Group\n",
    "Licensed under MIT License (see LICENSE.md)\n",
    "Author: Tae Ha Park (tpark94@stanford.edu)\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import _init_paths\n",
    "\n",
    "import easydict \n",
    "\n",
    "from core.config  import cfg, update_config\n",
    "from core.nets    import build_spnv2\n",
    "from core.dataset import get_dataloader\n",
    "from core.solver  import get_optimizer, get_scaler\n",
    "from core.engine.adapter    import do_adapt\n",
    "from core.engine.inference  import do_valid\n",
    "from core.utils.checkpoints import save_checkpoint\n",
    "from core.utils.utils import set_seeds_cudnn, setup_logger, create_logger_directories, \\\n",
    "                        load_camera_intrinsics, load_tango_3d_keypoints, \\\n",
    "                        write_model_info, num_trainable_parameters\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='TTDR')\n",
    "\n",
    "    args = easydict.EasyDict({\n",
    "      'cfg' : 'experiments/odr_phi3_B4_N1024.yaml',\n",
    "      'opts' : []\n",
    "      })\n",
    "\n",
    "\n",
    "    return args\n",
    "\n",
    "def main(cfg):\n",
    "    args = parse_args()\n",
    "    update_config(cfg, args)\n",
    "\n",
    "    # Set global seed, if not provided\n",
    "    seed = int(time.time()) if cfg.SEED is None else cfg.SEED\n",
    "\n",
    "    cfg.defrost()\n",
    "    cfg.SEED = seed\n",
    "    cfg.freeze()\n",
    "\n",
    "    _, output_dir, log_dir = \\\n",
    "        create_logger_directories(cfg, phase='train', write_cfg_to_file=True)\n",
    "\n",
    "    # No distributed training for TTDR\n",
    "    main_worker(\n",
    "        output_dir,\n",
    "        log_dir\n",
    "    )\n",
    "\n",
    "def main_worker(output_dir, log_dir):\n",
    "\n",
    "    # Set all seeds & cudNN\n",
    "    set_seeds_cudnn(cfg, seed=cfg.SEED)\n",
    "\n",
    "    # setup logger\n",
    "    logger = setup_logger(log_dir, 0, 'train', to_console=False)\n",
    "\n",
    "    # build network\n",
    "    model = build_spnv2(cfg)\n",
    "\n",
    "    # GPU device\n",
    "    device = torch.device('cuda')\n",
    "    model  = model.to(device)\n",
    "\n",
    "    # write model summary to file\n",
    "    write_model_info(model, log_dir)\n",
    "\n",
    "    # disable entire model grads\n",
    "    model.eval()\n",
    "    model.requires_grad_(False)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # For gradient accumulation with BatchNorm layers\n",
    "    # -------------------------------------------------------------\n",
    "    # Here, we manually update BatchNorm's running stats\n",
    "    def get_bn_features_from_name(name):\n",
    "        def bn_feature_hook(module, input, output):\n",
    "            bn_features[name] = input[0].detach()\n",
    "        return bn_feature_hook\n",
    "\n",
    "    bn_features = {}\n",
    "    handles = []\n",
    "    for n, m in model.backbone.named_modules():\n",
    "        if not cfg.MODEL.USE_GROUPNORM_BACKBONE and isinstance(m, torch.nn.BatchNorm2d):\n",
    "            # (1) Set BatchNorm layers to eval mode, so that running states are\n",
    "            #     not updated by every forward() calls\n",
    "            m.requires_grad_(True)\n",
    "            m.eval()\n",
    "\n",
    "            # (2) Keep the record of input features to BatchNorm layers\n",
    "            h = m.register_forward_hook(get_bn_features_from_name(n))\n",
    "            handles.append(h)\n",
    "\n",
    "        elif cfg.MODEL.USE_GROUPNORM_BACKBONE and isinstance(m, torch.nn.GroupNorm):\n",
    "            # GroupNorm layers -- simply allow requires_grad\n",
    "            m.requires_grad_(True)\n",
    "\n",
    "    logger.info(f'Total number of parameters with requires_grad=True')\n",
    "    logger.info(f'   - {num_trainable_parameters(model):,d}')\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = get_dataloader(cfg,\n",
    "                                  split='train',\n",
    "                                  distributed=False,\n",
    "                                  load_labels=False) # No labels during TTDR\n",
    "    val_loader   = get_dataloader(cfg,\n",
    "                                  split='val',\n",
    "                                  distributed=False,\n",
    "                                  load_labels=True)\n",
    "\n",
    "    # Optimizer & scaler for mixed-precision training\n",
    "    optimizer = get_optimizer(cfg, model)\n",
    "    scaler    = get_scaler(cfg)\n",
    "\n",
    "    # For validation\n",
    "    camera = load_camera_intrinsics(cfg.DATASET.CAMERA)\n",
    "    keypts_true_3D = load_tango_3d_keypoints(cfg.DATASET.KEYPOINTS)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Main ODR\n",
    "    # ---------------------------------------\n",
    "    # Single epoch training\n",
    "    do_adapt(0,\n",
    "             cfg,\n",
    "             model,\n",
    "             bn_features,\n",
    "             train_loader,\n",
    "             optimizer,\n",
    "             log_dir=log_dir,\n",
    "             device=device,\n",
    "             scaler=scaler)\n",
    "\n",
    "    # Remove hooks\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    # Validate on the fraction of dataset\n",
    "    # score = 0\n",
    "    score = do_valid(0,\n",
    "                    cfg,\n",
    "                    model,\n",
    "                    val_loader,\n",
    "                    camera,\n",
    "                    keypts_true_3D,\n",
    "                    valid_fraction=1.0,\n",
    "                    log_dir=output_dir,\n",
    "                    device=device)\n",
    "\n",
    "    # Save\n",
    "    save_checkpoint({\n",
    "        'epoch': 1,\n",
    "        'backbone': cfg.MODEL.BACKBONE.NAME,\n",
    "        'heads': cfg.MODEL.HEAD.NAMES,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_state_dict': model.state_dict(),\n",
    "        'best_score': score,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scaler': None\n",
    "    }, False, True, output_dir)\n",
    "\n",
    "    logger.info('\\n\\n')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}